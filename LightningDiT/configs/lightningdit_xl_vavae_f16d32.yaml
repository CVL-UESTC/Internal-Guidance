# we recommend to read config_details.yaml first.


ckpt_path: 'path/to/your/checkpoint.pt' # <---- download our pre-trained lightningdit or your own checkpoint

data:
  data_path: 'path/to/your/imagenet_train_256'  # <---- path to your data. it is generated by extract_features.py.
                                                #       if you just want to inference, download our latent_status.safetensors and give its folder path here is ok.
                                                #       the path should be like this:
                                                #       imagenet_train_256/
                                                #         ├── latents_rank00_shard000.safetensors (not required for inference)
                                                #         ├── latents_rank00_shard001.safetensors (not required for inference)
                                                #         ├── ...
                                                #         └── latents_stats.pt
  fid_reference_file: 'path/to/your/VIRTUAL_imagenet256_labeled.npz' # <---- path to your fid_reference_file.npz. download it from ADM

  # recommend to use default settings
  image_size: 256
  num_classes: 1000
  num_workers: 8
  latent_norm: true
  latent_multiplier: 1.0

# recommend to use default settings (we wil release codes with SD-VAE later)
vae:
  model_name: 'vavae_f16d32'
  downsample_ratio: 16

# recommend to use default settings
model:
  model_type: LightningDiT-XL/1
  encoder_depth: 8
  use_qknorm: false
  use_swiglu: true
  use_rope: true
  use_rmsnorm: true
  wo_shift: false
  in_chans: 32

# recommend to use default settings
train:
  max_steps: 3000000
  global_batch_size: 256 #256
  global_seed: 1024
  output_dir: 'output'
  exp_name: 'lightningdit_XL_IG_vavae_f16d32-doubelloss-8' # <---- experiment name, set as you like
  ckpt: null
  log_every: 100
  ckpt_every: 50000
  resume: True
optimizer:
  lr: 0.0002
  beta2: 0.95
  max_grad_norm: 1.0
  
# recommend to use default settings
transport:
  path_type: Linear
  prediction: velocity
  loss_weight: null
  sample_eps: null
  train_eps: null
  use_cosine_loss: true
  use_lognorm: False #true

# recommend to use default settings
sample:
  mode: ODE
  sampling_method: heun2
  atol: 0.000001
  rtol: 0.001
  reverse: false
  likelihood: false
  num_sampling_steps: 125
  cfg_scale: 1.45 # <---- cfg scale, for 680 epoch performance with FID=1.19 cfg_scale=1.45 ig_scale=1.4
  ig_scale: 1.4 # <---- ig scale, for 680 epoch performance with FID=1.34 ig_scale=1.7
  # recommend to use default settings
  per_proc_batch_size: 128
  fid_num: 50000
  cfg_interval_start: 0.19 
  timestep_shift: 0.3
